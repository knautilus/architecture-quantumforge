# Проектная работа 7 спринта

## Задание 1. Исследование моделей и инфраструктуры

### Сравнение LLM-моделей

| Параметр | Локальные модели (Hugging Face) | Облачные модели (OpenAI / YandexGPT) |
|----------|-------------------------------- |--------------------------------------|
| **Качество ответов** | Требуется тщательная настройка и дообучение | Модели уже оптимизированы. Высокое качество из коробки. Хорошо справляются с генерацией текстов, с контекстом и инструкциями |
| **Скорость работы** | Зависит от железа: на CPU работает медленно, на GPU быстрее | Скорость высокая и предсказуемая, так как облачные сервисы масштабируются автоматически |
| **Стоимость владения и использования** | Высокая: нужно покупать GPU, оплачивать электричество, обслуживание, поддерживать инфраструктуру, но меньшие расходы при высоком трафике | Оплата зависит от трафика. Нет расходов на железо, электроэнергию и поддержку инфраструктуры, но может стать дорого при большом объёме запросов |
| **Удобство и простота развёртывания** | Требует настройки окружения: библиотеки, Docker, оптимизация под CPU/GPU | Быстрое подключение через API: достаточно апи-ключа, минимальная настройка |

### Сравните моделей эмбеддингов

| Параметр | Локальные Sentence-Transformers | Облачные OpenAI Embeddings |
|----------|---------------------------------|----------------------------|
| **Скорость создания индекса** | Быстро на GPU, медленно на CPU| Высокая скорость |
| **Качество поиска** | Хорошее, можно дообучать | Высокое качество|
| **Стоимость владения и использования** | Значительные начальные затраты: железо + ПО. Высокие затраты на электроэнергию | Плата только за доступ к API, но стоимость растёт при больших объёмах запросов |

### Сравнение векторных баз ChromaDB и FAISS

| Параметр | ChromaDB | FAISS |
|----------|----------|-------|
| **Скорость поиска и индексации** | Обеспечивает быстрый и эффективный поиск, но скорость обработки массивных наборов данных меньше, чем у FAISS | Высокая. Оптимизирован для крупномасштабного многомерного векторного поиска |
| **Сложность внедрения и поддержки** | Легковесная, нативная для Python и проста в самохостинге или локальном запуске | Простая библиотека. Требует ручного управления персистентностью |
| **Удобство в работе** | Предоставляет высокоуровневый API, который устраняет большую часть сложностей и упрощает его использование. Поиск по метаданным из коробки | Низкоуровневая библиотека. Нужно будет самостоятельно управлять персистентностью, хранением, метаданными |
| **Стоимость владения (учёт инфраструктуры)** | Open-source. Для больших проектов может понадобиться мощный сервер или облако | Open-source. Развертывание на больших данных требует хорошее железо (CPU/GPU) |

### Выводы

Использование облачных моделей нежелательно для основного контура из-за конфиденциальности данных.

Локальные модели - оптимальный вариант.

Облачные модели можно рассмотреть для прототипирования и для обработки публичных данных.

FAISS отлично подходит для исследований и демонстраций: простота, высокая производительность.

Но если нужен поиск по метаданным из коробки и если не охота разбираться с персистентностью, то можно посмотреть в сторону ChromaDB.

### Рекомендуемая конфигурация сервера

Для локального RAG-бота с Sentence-Transformers + FAISS + HuggingFace LLM:
- CPU: 8-16 ядер
- RAM: 32-64 GB
- GPU: NVIDIA RTX 3060 или выше
- Хранилище: SSD, от 500 GB
