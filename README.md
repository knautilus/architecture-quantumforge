# Проектная работа 7 спринта

## Задание 1. Исследование моделей и инфраструктуры

### Сравнение LLM-моделей

| Параметр | Локальные модели (Hugging Face) | Облачные модели (OpenAI / YandexGPT) |
|----------|-------------------------------- |--------------------------------------|
| **Качество ответов** | Требуется тщательная настройка и дообучение | Модели уже оптимизированы. Высокое качество из коробки. Хорошо справляются с генерацией текстов, с контекстом и инструкциями |
| **Скорость работы** | Зависит от железа: на CPU работает медленно, на GPU быстрее | Скорость высокая и предсказуемая, так как облачные сервисы масштабируются автоматически |
| **Стоимость владения и использования** | Высокая: нужно покупать GPU, оплачивать электричество, обслуживание, поддерживать инфраструктуру, но меньшие расходы при высоком трафике | Оплата зависит от трафика. Нет расходов на железо, электроэнергию и поддержку инфраструктуры, но может стать дорого при большом объёме запросов |
| **Удобство и простота развёртывания** | Требует настройки окружения: библиотеки, Docker, оптимизация под CPU/GPU | Быстрое подключение через API: достаточно апи-ключа, минимальная настройка |

### Сравните моделей эмбеддингов

| Параметр | Локальные Sentence-Transformers | Облачные OpenAI Embeddings |
|----------|---------------------------------|----------------------------|
| **Скорость создания индекса** | Быстро на GPU, медленно на CPU| Высокая скорость |
| **Качество поиска** | Хорошее, можно дообучать | Высокое качество|
| **Стоимость владения и использования** | Значительные начальные затраты: железо + ПО. Высокие затраты на электроэнергию | Плата только за доступ к API, но стоимость растёт при больших объёмах запросов |

### Сравнение векторных баз ChromaDB и FAISS

| Параметр | ChromaDB | FAISS |
|----------|----------|-------|
| **Скорость поиска и индексации** | Обеспечивает быстрый и эффективный поиск, но скорость обработки массивных наборов данных меньше, чем у FAISS | Высокая. Оптимизирован для крупномасштабного многомерного векторного поиска |
| **Сложность внедрения и поддержки** | Легковесная, нативная для Python и проста в самохостинге или локальном запуске | Простая библиотека. Требует ручного управления персистентностью |
| **Удобство в работе** | Предоставляет высокоуровневый API, который устраняет большую часть сложностей и упрощает его использование. Поиск по метаданным из коробки | Низкоуровневая библиотека. Нужно будет самостоятельно управлять персистентностью, хранением, метаданными |
| **Стоимость владения (учёт инфраструктуры)** | Open-source. Для больших проектов может понадобиться мощный сервер или облако | Open-source. Развертывание на больших данных требует хорошее железо (CPU/GPU) |

### Сравнение конфигураций

| Вариант | Описание | Преимущества | Недостатки |
| ------- | -------- | ------------ | ---------- |
| **Облачный** | Cloud OpenAI GPT-4 + OpenAI Embeddings + FAISS/ChromaDB в Docker | Быстрый запуск, высокое качество | Высокая стоимость при росте запросов |
| **Гибридный** | Hybrid OpenAI Embeddings + локальная модель (например, Llama 3 8B) + ChromaDB | Баланс между качеством и контролем над данными | Средняя сложность, зависит от GPU |
| **Локальный** | Всё локально: Mistral + Sentence-Transformers + FAISS/ChromaDB | Полный контроль и безопасность | Сложное и дорогое развёртывание, ниже качество |

### Выводы

Использование облачных моделей нежелательно для основного контура из-за конфиденциальности данных.

Локальные модели - оптимальный вариант.

Облачные модели можно рассмотреть для прототипирования и для обработки публичных данных.

FAISS отлично подходит для исследований и демонстраций: простота, высокая производительность.

Но если нужен поиск по метаданным из коробки и если не охота разбираться с персистентностью, то можно посмотреть в сторону ChromaDB.

### Рекомендуемая конфигурация сервера

Для локального RAG-бота с Sentence-Transformers + FAISS + HuggingFace LLM:
- CPU: 8-16 ядер
- RAM: 32-64 GB
- GPU: NVIDIA RTX 3060 или выше
- Хранилище: SSD, от 500 GB

## Задание 2. Подготовка базы знаний

Для построения базы знаний был взят фандом по серии игр "Древние Свитки": https://elderscrolls.fandom.com/ru/wiki/The_Elder_Scrolls_Wiki

В папке `knowledge_base\original` лежат исходные документы.

В файле `knowledge_base\terms_map.json` описан словарь замен. Заменены имена, названия провинций, рас, континентов, планет, некоторых событий; слово "свиток" заменено на "манускрипт".

Чтобы получить базу с замененными словами, надо в папке `knowledge_base` выполнить команду `python replace.py`

База с замененными словами находится в папке `knowledge_base\processed`

## Задание 3. Создание векторного индекса базы знаний

Были опробованы несколько эмбеддинг-моделей: `BAAI/bge-base-en-v1.5`, `BAAI/bge-m3`, `cointegrated/rubert-tiny2`, `sentence-transformers/all-MiniLM-L6-v2`, s`entence-transformers/all-mpnet-base-v2`, `ai-forever/sbert_large_mt_nlu_ru`, `DeepPavlov/rubert-base-cased-sentence`.
При поиске фраз модель `BAAI/bge-base-en-v1.5` дала лучший результат по точности.

Название модели: BAAI/bge-base-en-v1.5
Ссылка: https://huggingface.co/BAAI/bge-base-en-v1.5
Тип: Локальная, Sentence-Transformers
Размер эмбеддингов: 768
Размер модели: ~438 МБ

Использовался Python версии 3.10.11.

Перед созданием индексов необходимо настроить окружение:

```
python -m venv .venv
source .venv/bin/activate
pip install langchain faiss-cpu
pip install faiss-cpu
pip install langchain-experimental
pip install sentence-transformers
```

Скрипт для создания индекса:

`python build_index.py`

В результате получилось 420. Чанки создались моментально - около секунды. Сам индекс создавался около минуты.

Скрипт для поиска фразы "кризис подливиона":

`python search_index.py`

Результат поиска:

```
Результат 1:
Источник: ./knowledge_base/processed\Лукоморье.txt, чанк: chunk_0198.
Текст результата:
Во время Кризиса Подливиона, хисты призвали рептиан назад на родину, чтобы те противостояли вторжению даллегра. Сами рептиане сражались за свою землю так яростно, что генералы армии Дагона сами стали закрывать врата. В начале Четвёртой эры, после окончания Кризиса Подливиона, члены националистической партии Ан-Зайлиль пришли к власти в Лукоморье и одни из первых объявили о независимости провинции от Империи Тутриэля
--------------------------------------------------
Результат 2:
Источник: ./knowledge_base/processed\Кризис подливиона.txt, чанк: chunk_0150.
Текст результата:
Причиной Кризиса стал даллегрический культ Мифический Рассвет, члены которого убили последнего императора династии Пупкинов — Уриэля Пупкина VII и всех его законных наследников. Практически сразу после этого по всему континенту стали открываться Врата Подливиона (порталы, соединяющие мир смертных с Планом Степана Дагона — Мёртвыми Землями), из которых хлынули полчища даллегра
--------------------------------------------------
Результат 3:
Источник: ./knowledge_base/processed\Кризис подливиона.txt, чанк: chunk_0149.
Текст результата:
Кризис Подливиона, Великая Боль, Великая Тоска — масштабная война между Империей Тутриэля и армиями Даллегрического Принца Разрушения Степана Дагона. Кризис Подливиона начался 27 числа месяца Последнего Зерна 433 года Третьей эры. Эта война стала концом Третьей эры и началом Четвёртой, а также послужила причиной падения Империи Пупкинов
```

## Задание 4. Реализация RAG-бота с техниками промптинга

