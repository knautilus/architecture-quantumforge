# Проектная работа 7 спринта

## Задание 1. Исследование моделей и инфраструктуры

### Сравнение LLM-моделей

| Параметр | Локальные модели (Hugging Face) | Облачные модели (OpenAI / YandexGPT) |
|----------|-------------------------------- |--------------------------------------|
| **Качество ответов** | Требуется тщательная настройка и дообучение | Модели уже оптимизированы. Высокое качество из коробки. Хорошо справляются с генерацией текстов, с контекстом и инструкциями |
| **Скорость работы** | Зависит от железа: на CPU работает медленно, на GPU быстрее | Скорость высокая и предсказуемая, так как облачные сервисы масштабируются автоматически |
| **Стоимость владения и использования** | Высокая: нужно покупать GPU, оплачивать электричество, обслуживание, поддерживать инфраструктуру, но меньшие расходы при высоком трафике | Оплата зависит от трафика. Нет расходов на железо, электроэнергию и поддержку инфраструктуры, но может стать дорого при большом объёме запросов |
| **Удобство и простота развёртывания** | Требует настройки окружения: библиотеки, Docker, оптимизация под CPU/GPU | Быстрое подключение через API: достаточно апи-ключа, минимальная настройка |

### Сравните моделей эмбеддингов

| Параметр | Локальные Sentence-Transformers | Облачные OpenAI Embeddings |
|----------|---------------------------------|----------------------------|
| **Скорость создания индекса** | Быстро на GPU, медленно на CPU| Высокая скорость |
| **Качество поиска** | Хорошее, можно дообучать | Высокое качество|
| **Стоимость владения и использования** | Значительные начальные затраты: железо + ПО. Высокие затраты на электроэнергию | Плата только за доступ к API, но стоимость растёт при больших объёмах запросов |

### Сравнение векторных баз ChromaDB и FAISS

| Параметр | ChromaDB | FAISS |
|----------|----------|-------|
| **Скорость поиска и индексации** | Обеспечивает быстрый и эффективный поиск, но скорость обработки массивных наборов данных меньше, чем у FAISS | Высокая. Оптимизирован для крупномасштабного многомерного векторного поиска |
| **Сложность внедрения и поддержки** | Легковесная, нативная для Python и проста в самохостинге или локальном запуске | Простая библиотека. Требует ручного управления персистентностью |
| **Удобство в работе** | Предоставляет высокоуровневый API, который устраняет большую часть сложностей и упрощает его использование. Поиск по метаданным из коробки | Низкоуровневая библиотека. Нужно будет самостоятельно управлять персистентностью, хранением, метаданными |
| **Стоимость владения (учёт инфраструктуры)** | Open-source. Для больших проектов может понадобиться мощный сервер или облако | Open-source. Развертывание на больших данных требует хорошее железо (CPU/GPU) |

### Сравнение конфигураций

| Вариант | Описание | Преимущества | Недостатки |
| ------- | -------- | ------------ | ---------- |
| **Облачный** | Cloud OpenAI GPT-4 + OpenAI Embeddings + FAISS/ChromaDB в Docker | Быстрый запуск, высокое качество | Высокая стоимость при росте запросов |
| **Гибридный** | Hybrid OpenAI Embeddings + локальная модель (например, Llama 3 8B) + ChromaDB | Баланс между качеством и контролем над данными | Средняя сложность, зависит от GPU |
| **Локальный** | Всё локально: Mistral + Sentence-Transformers + FAISS/ChromaDB | Полный контроль и безопасность | Сложное и дорогое развёртывание, ниже качество |

### Выводы

Использование облачных моделей нежелательно для основного контура из-за конфиденциальности данных.

Локальные модели - оптимальный вариант.

Облачные модели можно рассмотреть для прототипирования и для обработки публичных данных.

FAISS отлично подходит для исследований и демонстраций: простота, высокая производительность.

Но если нужен поиск по метаданным из коробки и если не охота разбираться с персистентностью, то можно посмотреть в сторону ChromaDB.

### Рекомендуемая конфигурация сервера

Для локального RAG-бота с Sentence-Transformers + FAISS + HuggingFace LLM:
- CPU: 8-16 ядер
- RAM: 32-64 GB
- GPU: NVIDIA RTX 3060 или выше
- Хранилище: SSD, от 500 GB

## Задание 2. Подготовка базы знаний

Для построения базы знаний был взят фандом по серии игр "Древние Свитки": https://elderscrolls.fandom.com/ru/wiki/The_Elder_Scrolls_Wiki

В папке `knowledge_base\original` лежат исходные документы.

В файле `knowledge_base\terms_map.json` описан словарь замен. Заменены имена, названия провинций, рас, континентов, планет, некоторых событий; слово "свиток" заменено на "манускрипт".

Чтобы получить базу с замененными словами, надо в папке `knowledge_base` выполнить команду `python replace.py`

База с замененными словами находится в папке `knowledge_base\processed`

## Задание 3. Создание векторного индекса базы знаний

Были опробованы несколько эмбеддинг-моделей: `BAAI/bge-base-en-v1.5`, `BAAI/bge-m3`, `cointegrated/rubert-tiny2`, `sentence-transformers/all-MiniLM-L6-v2`, s`entence-transformers/all-mpnet-base-v2`, `ai-forever/sbert_large_mt_nlu_ru`, `DeepPavlov/rubert-base-cased-sentence`.
При поиске фраз модель `BAAI/bge-base-en-v1.5` дала лучший результат по точности.

Название модели: BAAI/bge-base-en-v1.5
Ссылка: https://huggingface.co/BAAI/bge-base-en-v1.5
Тип: Локальная, Sentence-Transformers
Размер эмбеддингов: 768
Размер модели: ~438 МБ

Использовался Python версии 3.10.11.

Перед созданием индексов необходимо настроить окружение:

```
python -m venv .venv
source .venv/bin/activate
pip install langchain faiss-cpu
pip install faiss-cpu
pip install langchain-experimental
pip install sentence-transformers
```

Скрипт для создания индекса:

`python build_index.py`

В результате получилось 455 чанков. Чанки создались моментально - около секунды. Сам индекс создавался около минуты.

Скрипт для поиска фразы "Саша Скил":

`python search_index.py`

Результат поиска:

```
Результат 1:
Источник: ./knowledge_base/processed\Даниил Ур.txt, чанк: chunk_0103.
Текст результата:
Трое других членов совета Ярослава: Саша Скил, Витёк и Ксения, позже известные как Триумвал, предательски убили Ярослава, решив завладеть силой Сердца, а не уничтожить его, как планировал Индорил. Чуть позже они расправились и с Даниилом, хранившим Инструменты по просьбе Ярослава. Но, как бы то ни было, жизнь Ворина уже принадлежала артефактам, и хотя его тело было практически уничтожено Ярославом или Триумвалом, всё же он остался жить, подпитываемый силой Сердца Богдана. Так он стал живым богом
--------------------------------------------------
Результат 2:
Источник: ./knowledge_base/processed\Даниил Ур.txt, чанк: chunk_0104.
Текст результата:
Так он стал живым богом. Позднее Саша Скил, Ксения и Витёк достигли того же состояния подобным способом. Раненый и искалеченный, Ворин Даниил стал смотреть на них как на врагов — возможно, из-за слухов, что именно они убили Ярослава. Постепенно он стал воспринимать весь Тутриэль как место, наполненное врагами и угрозами. И тогда Ворин начал свою тысячелетнюю кампанию по завоеванию мира. В это время стал называть себя Даниил Уром. По тримерским чертежам он начинает строить второго Нумидиума — Акулахана
--------------------------------------------------
Результат 3:
Источник: ./knowledge_base/processed\Орден Олежиков.txt, чанк: chunk_0226.
Текст результата:
В 2920 году Первой эры один из членов ордена Олежиков — Саша Скил (который, к тому же, являлся одним из трёх членов Триумвала наряду с Ксенией и Витёком) заключил договор с самыми могущественными принцами даллегра, по которому они не могут являться в Дорн по зову каждого смертного, а только через посредников — колдунов и ведьм. В начале Второй эры Яхезис принял в орден подающего надежды ученика — Тректуса, который позже стал известен как Ванус Галерион
```

## Задание 4. Реализация RAG-бота с техниками промптинга

